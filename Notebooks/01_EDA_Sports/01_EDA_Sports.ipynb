{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3fdfbc1",
   "metadata": {},
   "source": [
    "# 01_EDA_Sports.ipynb — Sports Performance & Injury Risk Analytics\n",
    "\n",
    "**Goal:** Perform comprehensive exploratory analysis on football (soccer) player, training, match, wellness, and injury data to understand workload dynamics, wellness correlations, and injury risk drivers.\n",
    "\n",
    "**Data sources (synthetic):**\n",
    "- `Players.csv`, `Teams.csv`, `Matches.csv`\n",
    "- `PlayerMatchStats.csv`, `TrainingSessions.csv`, `WellnessSnapshots.csv`, `Injuries.csv`\n",
    "\n",
    "**Outputs:**\n",
    "- Cleaned analytical summaries saved to `../data/` for modeling\n",
    "- Visuals covering demographics, workload, performance, injuries, and integrated insights\n",
    "\n",
    "*Note:* This notebook uses **matplotlib only** for plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f64a249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV_DIR: D:\\Sports Performance & Injury Risk Analytics\\Notebooks\\Sports Performance & Injury Risk Analytics\\DataSet\n",
      "DATA_OUT: D:\\Sports Performance & Injury Risk Analytics\\Notebooks\\data\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Imports & setup ---\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display defaults\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "plt.rcParams.update({\n",
    "    \"figure.figsize\": (10, 5),\n",
    "    \"axes.grid\": True\n",
    "})\n",
    "\n",
    "# Paths\n",
    "BASE = Path(\".\").resolve()\n",
    "CSV_DIR = (BASE / \"..\" / \"Sports Performance & Injury Risk Analytics\" / \"DataSet\").resolve()   # adjust if needed\n",
    "DATA_OUT = (BASE / \"..\" / \"data\").resolve()\n",
    "DATA_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"CSV_DIR:\", CSV_DIR)\n",
    "print(\"DATA_OUT:\", DATA_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ff253",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\Sports Performance & Injury Risk Analytics\\\\Notebooks\\\\Sports Performance & Injury Risk Analytics\\\\DataSet\\\\Players.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- 2. Load data ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m players = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCSV_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPlayers.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m teams = pd.read_csv(CSV_DIR / \u001b[33m\"\u001b[39m\u001b[33mTeams.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m matches = pd.read_csv(CSV_DIR / \u001b[33m\"\u001b[39m\u001b[33mMatches.csv\u001b[39m\u001b[33m\"\u001b[39m, parse_dates=[\u001b[33m\"\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'D:\\\\Sports Performance & Injury Risk Analytics\\\\Notebooks\\\\Sports Performance & Injury Risk Analytics\\\\DataSet\\\\Players.csv'"
     ]
    }
   ],
   "source": [
    "# --- 2. Load data ---\n",
    "players = pd.read_csv(CSV_DIR / \"Players.csv\")\n",
    "teams = pd.read_csv(CSV_DIR / \"Teams.csv\")\n",
    "matches = pd.read_csv(CSV_DIR / \"Matches.csv\", parse_dates=[\"Date\"])\n",
    "pms = pd.read_csv(CSV_DIR / \"PlayerMatchStats.csv\")\n",
    "train = pd.read_csv(CSV_DIR / \"TrainingSessions.csv\", parse_dates=[\"Date\"])\n",
    "well = pd.read_csv(CSV_DIR / \"WellnessSnapshots.csv\", parse_dates=[\"SnapshotDate\"])\n",
    "inj = pd.read_csv(CSV_DIR / \"Injuries.csv\", parse_dates=[\"InjuryDate\",\"RecoveryDate\"])\n",
    "\n",
    "shapes = {\n",
    "    \"Players\": players.shape,\n",
    "    \"Teams\": teams.shape,\n",
    "    \"Matches\": matches.shape,\n",
    "    \"PlayerMatchStats\": pms.shape,\n",
    "    \"TrainingSessions\": train.shape,\n",
    "    \"WellnessSnapshots\": well.shape,\n",
    "    \"Injuries\": inj.shape\n",
    "}\n",
    "print(\"Shapes:\", shapes)\n",
    "\n",
    "date_span = f\"{train['Date'].min().date()} to {train['Date'].max().date()}\" if len(train) else \"N/A\"\n",
    "print(\"Training date span:\", date_span)\n",
    "\n",
    "# Basic NA check\n",
    "na_summary = {\n",
    "    \"Players\": players.isna().sum().to_dict(),\n",
    "    \"Teams\": teams.isna().sum().to_dict(),\n",
    "    \"Matches\": matches.isna().sum().to_dict(),\n",
    "    \"PlayerMatchStats\": pms.isna().sum().to_dict(),\n",
    "    \"TrainingSessions\": train.isna().sum().to_dict(),\n",
    "    \"WellnessSnapshots\": well.isna().sum().to_dict(),\n",
    "    \"Injuries\": inj.isna().sum().to_dict(),\n",
    "}\n",
    "na_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb1d53d",
   "metadata": {},
   "source": [
    "## 3. Player Demographics & Team Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f57288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position distribution\n",
    "pos_counts = players[\"Position\"].value_counts().sort_index()\n",
    "plt.figure()\n",
    "pos_counts.plot(kind=\"bar\")\n",
    "plt.title(\"Players by Position\"); plt.xlabel(\"Position\"); plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Age distribution\n",
    "plt.figure()\n",
    "players[\"Age\"].plot(kind=\"hist\", bins=20)\n",
    "plt.title(\"Age Distribution\"); plt.xlabel(\"Age\"); plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Height vs Weight scatter (colored by position via markers)\n",
    "pos_markers = {\"GK\":\"o\",\"DF\":\"s\",\"MF\":\"^\",\"FW\":\"x\"}\n",
    "plt.figure()\n",
    "for pos, mk in pos_markers.items():\n",
    "    sub = players[players[\"Position\"]==pos]\n",
    "    plt.scatter(sub[\"HeightCM\"], sub[\"WeightKG\"], label=pos, marker=mk, alpha=0.7)\n",
    "plt.title(\"Height vs Weight by Position\"); plt.xlabel(\"Height (cm)\"); plt.ylabel(\"Weight (kg)\")\n",
    "plt.legend(); plt.show()\n",
    "\n",
    "# BMI & InjuryProneScore\n",
    "players[\"BMI\"] = players[\"WeightKG\"] / (players[\"HeightCM\"]/100.0)**2\n",
    "plt.figure()\n",
    "plt.scatter(players[\"BMI\"], players[\"InjuryProneScore\"], alpha=0.6)\n",
    "plt.title(\"InjuryProneScore vs BMI\"); plt.xlabel(\"BMI\"); plt.ylabel(\"InjuryProneScore\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Average BMI:\", round(players[\"BMI\"].mean(), 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f7efa4",
   "metadata": {},
   "source": [
    "## 4. Training Load & Wellness Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7aa8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training features\n",
    "train = train.sort_values([\"PlayerID\",\"Date\"]).copy()\n",
    "train[\"Load\"] = train[\"DistanceKM\"] + 0.5*(train[\"DurationMinutes\"]/60.0)\n",
    "\n",
    "# Rolling loads\n",
    "train[\"Acute7\"] = train.groupby(\"PlayerID\")[\"Load\"].rolling(7, min_periods=1).sum().reset_index(0,drop=True)\n",
    "train[\"Chronic28\"] = train.groupby(\"PlayerID\")[\"Load\"].rolling(28, min_periods=7).sum().reset_index(0,drop=True)\n",
    "train[\"ACWR\"] = train[\"Acute7\"] / train[\"Chronic28\"].replace({0: np.nan})\n",
    "\n",
    "# Plot ACWR distribution\n",
    "plt.figure()\n",
    "train[\"ACWR\"].plot(kind=\"hist\", bins=40)\n",
    "plt.title(\"ACWR Distribution\"); plt.xlabel(\"ACWR\"); plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Wellness correlations (numeric only)\n",
    "well_num = well[[\"SleepHours\",\"StressLevel\",\"MuscleSoreness\",\"HydrationLevel\",\"RecoveryScore\"]].copy()\n",
    "corr = well_num.corr(numeric_only=True)\n",
    "print(\"Wellness correlations:\\n\", corr.round(2))\n",
    "\n",
    "# Visualize correlation matrix with matplotlib (no seaborn)\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "cax = ax.imshow(corr.values, cmap=\"viridis\")\n",
    "ax.set_xticks(range(len(corr.columns))); ax.set_yticks(range(len(corr.columns)))\n",
    "ax.set_xticklabels(corr.columns, rotation=45, ha=\"right\"); ax.set_yticklabels(corr.columns)\n",
    "fig.colorbar(cax, ax=ax)\n",
    "ax.set_title(\"Wellness Correlation Matrix\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Weekly averages of wellness\n",
    "well[\"WeekStart\"] = well[\"SnapshotDate\"] - pd.to_timedelta(well[\"SnapshotDate\"].dt.weekday, unit=\"D\")\n",
    "weekly_well = (well.groupby(\"WeekStart\")[[\"SleepHours\",\"StressLevel\",\"RecoveryScore\"]].mean().reset_index())\n",
    "plt.figure()\n",
    "plt.plot(weekly_well[\"WeekStart\"], weekly_well[\"SleepHours\"], label=\"SleepHours\")\n",
    "plt.plot(weekly_well[\"WeekStart\"], weekly_well[\"StressLevel\"], label=\"StressLevel\")\n",
    "plt.plot(weekly_well[\"WeekStart\"], weekly_well[\"RecoveryScore\"], label=\"RecoveryScore\")\n",
    "plt.title(\"Weekly Wellness Trends\"); plt.xlabel(\"WeekStart\"); plt.ylabel(\"Value\"); plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe1ad05",
   "metadata": {},
   "source": [
    "## 5. Match Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a710db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge position onto match stats\n",
    "pms_pos = pms.merge(players[[\"PlayerID\",\"Position\"]], on=\"PlayerID\", how=\"left\")\n",
    "\n",
    "# Avg distance by position\n",
    "avg_dist_pos = pms_pos.groupby(\"Position\")[\"DistanceCoveredKM\"].mean().reindex([\"GK\",\"DF\",\"MF\",\"FW\"])\n",
    "plt.figure()\n",
    "avg_dist_pos.plot(kind=\"bar\")\n",
    "plt.title(\"Avg Match Distance by Position (KM)\"); plt.xlabel(\"Position\"); plt.ylabel(\"KM\")\n",
    "plt.show()\n",
    "\n",
    "# Pass accuracy vs match rating (scatter)\n",
    "plt.figure()\n",
    "plt.scatter(pms[\"PassAccuracyPct\"], pms[\"MatchRating\"], alpha=0.4)\n",
    "plt.title(\"Pass Accuracy vs Match Rating\"); plt.xlabel(\"PassAccuracy (%)\"); plt.ylabel(\"Match Rating\")\n",
    "plt.show()\n",
    "\n",
    "# Minutes played trend (seasonal)\n",
    "matches[\"Season\"] = matches[\"Date\"].dt.year\n",
    "pms_season = pms.merge(matches[[\"MatchID\",\"Season\"]], on=\"MatchID\", how=\"left\")\n",
    "mins_season = pms_season.groupby(\"Season\")[\"MinutesPlayed\"].sum()\n",
    "plt.figure()\n",
    "mins_season.plot(kind=\"bar\")\n",
    "plt.title(\"Total Minutes Played by Season\"); plt.xlabel(\"Season\"); plt.ylabel(\"Minutes\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2575ac9",
   "metadata": {},
   "source": [
    "## 6. Injury Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcfe3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Injuries by severity and season\n",
    "inj[\"Season\"] = inj[\"InjuryDate\"].dt.year\n",
    "cnt_sev = inj[\"Severity\"].value_counts()\n",
    "plt.figure()\n",
    "cnt_sev.plot(kind=\"bar\")\n",
    "plt.title(\"Injuries by Severity\"); plt.xlabel(\"Severity\"); plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "inj_season = inj.groupby(\"Season\").size()\n",
    "plt.figure()\n",
    "inj_season.plot(kind=\"bar\")\n",
    "plt.title(\"Injuries per Season\"); plt.xlabel(\"Season\"); plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Days out distribution\n",
    "plt.figure()\n",
    "inj[\"DaysOut\"].plot(kind=\"hist\", bins=30)\n",
    "plt.title(\"Days Out Distribution\"); plt.xlabel(\"Days\"); plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Injury incidence per 1000 training hours\n",
    "total_hours = train[\"DurationMinutes\"].sum()/60.0 if len(train) else 0.0\n",
    "rate = (len(inj) / total_hours * 1000) if total_hours>0 else np.nan\n",
    "print(f\"Injury rate: {rate:.2f} per 1000 training hours\")\n",
    "\n",
    "# Injury vs ACWR band incidence\n",
    "def acwr_band(x):\n",
    "    if pd.isna(x): return \"NA\"\n",
    "    if x < 0.8: return \"<0.8\"\n",
    "    if x < 1.3: return \"0.8–1.3\"\n",
    "    if x < 1.5: return \"1.3–1.5\"\n",
    "    return \">=1.5\"\n",
    "\n",
    "train[\"ACWRBand\"] = train[\"ACWR\"].apply(acwr_band)\n",
    "inj_daily = inj[[\"PlayerID\",\"InjuryDate\"]].copy().rename(columns={\"InjuryDate\":\"Date\"})\n",
    "inj_daily[\"Inj\"] = 1\n",
    "merged = pd.merge(train[[\"PlayerID\",\"Date\",\"ACWRBand\"]], inj_daily, on=[\"PlayerID\",\"Date\"], how=\"left\")\n",
    "incidence = merged.groupby(\"ACWRBand\")[\"Inj\"].mean().reindex([\"<0.8\",\"0.8–1.3\",\"1.3–1.5\",\">=1.5\",\"NA\"])\n",
    "plt.figure()\n",
    "incidence.plot(kind=\"bar\")\n",
    "plt.title(\"Injury Incidence by ACWR Band\"); plt.xlabel(\"ACWR Band\"); plt.ylabel(\"Incidence\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b38af64",
   "metadata": {},
   "source": [
    "## 7. Integrated Insights & Summary Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072de76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Player-level summary for modeling & BI\n",
    "summary = (\n",
    "    pms.groupby(\"PlayerID\")\n",
    "       .agg(TotalMinutes=(\"MinutesPlayed\",\"sum\"),\n",
    "            AvgMatchRating=(\"MatchRating\",\"mean\"),\n",
    "            AvgDistanceKM=(\"DistanceCoveredKM\",\"mean\"),\n",
    "            AvgPassAcc=(\"PassAccuracyPct\",\"mean\"),\n",
    "            TotalSprints=(\"Sprints\",\"sum\"))\n",
    "       .reset_index()\n",
    "       .merge(players[[\"PlayerID\",\"TeamID\",\"Position\",\"InjuryProneScore\",\"Age\"]], on=\"PlayerID\", how=\"left\")\n",
    ")\n",
    "\n",
    "inj_counts = inj.groupby(\"PlayerID\").size().reset_index(name=\"InjuryCount\")\n",
    "summary = summary.merge(inj_counts, on=\"PlayerID\", how=\"left\").fillna({\"InjuryCount\":0})\n",
    "\n",
    "summary[\"MinutesPerInjury\"] = summary[\"TotalMinutes\"] / (summary[\"InjuryCount\"] + 1)\n",
    "\n",
    "print(\"Summary shape:\", summary.shape)\n",
    "summary.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ee1726",
   "metadata": {},
   "source": [
    "## 8. Save Analytical Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e358334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned/engineered datasets for modeling\n",
    "summary_path = DATA_OUT / \"player_summary_for_model.csv\"\n",
    "train_out_path = DATA_OUT / \"train_features_prepared.csv\"\n",
    "\n",
    "summary.to_csv(summary_path, index=False)\n",
    "train.to_csv(train_out_path, index=False)\n",
    "\n",
    "print(\"Saved:\", summary_path)\n",
    "print(\"Saved:\", train_out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f543db",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "- Build `02_InjuryRisk_Model.ipynb` (feature engineering for rolling loads, wellness trends; train Logistic/RandomForest/XGBoost).\n",
    "- Create Power BI report with pages: **Squad Overview**, **Load vs Fatigue**, **Injury Risk & Alerts**.\n",
    "- Write SQL views for model serving (`InjuryRiskScores`) and integrate back into the BI layer.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
